{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreditClass: Master Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis (EDA) and model comparison for credit classification using the UCI German Credit dataset.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Introduction** - Project overview and dataset description\n",
    "2. **Data Loading** - Download and inspect raw data\n",
    "3. **Exploratory Data Analysis** - Distributions, correlations, class balance\n",
    "4. **Preprocessing** - Encoding and target derivation demonstration\n",
    "5. **Model Comparison** - Train and compare all six models\n",
    "6. **Results** - Comparison tables, charts, ROC overlay\n",
    "7. **Interpretability** - SHAP comparison across models\n",
    "8. **Multi-Task Analysis** - Results for tier classification and loan approval\n",
    "9. **Conclusions** - Summary and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "**CreditClass** demonstrates classification techniques on real-world credit data. We showcase six different models:\n",
    "\n",
    "| Model | Type | Key Characteristic |\n",
    "|-------|------|-------------------|\n",
    "| Logistic Regression | Linear | Interpretable baseline |\n",
    "| Random Forest | Ensemble (Bagging) | Robust, handles non-linearity |\n",
    "| XGBoost | Ensemble (Boosting) | State-of-the-art performance |\n",
    "| SVM | Kernel-based | Maximum margin classifier |\n",
    "| k-NN | Instance-based | Simple, non-parametric |\n",
    "| Neural Network | Deep Learning | Learns complex patterns |\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The **UCI German Credit** dataset contains 1,000 samples with 20 features describing credit applicants. The task is to classify applicants as good or bad credit risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from creditclass.preprocessing import (\n",
    "    download_data,\n",
    "    load_data,\n",
    "    encode_categoricals,\n",
    "    create_default_target,\n",
    "    create_tier_target,\n",
    "    create_approval_target,\n",
    "    split_data,\n",
    "    prepare_data,\n",
    "    NUMERICAL_COLUMNS,\n",
    "    CATEGORICAL_COLUMNS,\n",
    ")\n",
    "from creditclass.feature_engineering import engineer_all_features\n",
    "from creditclass.training import get_model, train_model, save_model, get_all_model_names\n",
    "from creditclass.evaluation import (\n",
    "    evaluate_model,\n",
    "    compare_models,\n",
    "    compute_shap_values,\n",
    "    get_feature_importance,\n",
    ")\n",
    "from creditclass.plots import (\n",
    "    set_plot_style,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_roc_curves_comparison,\n",
    "    plot_precision_recall,\n",
    "    plot_feature_importance,\n",
    "    plot_correlation_heatmap,\n",
    "    plot_class_distribution,\n",
    "    plot_distribution,\n",
    "    plot_model_comparison,\n",
    "    plot_metrics_grouped_bar,\n",
    "    plot_shap_summary,\n",
    "    plot_calibration,\n",
    "    COLOURS,\n",
    ")\n",
    "\n",
    "set_plot_style()\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load data\n",
    "download_data()\n",
    "df = load_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {df.shape[1] - 1}\")\n",
    "print(f\"  - Numerical: {len(NUMERICAL_COLUMNS)}\")\n",
    "print(f\"  - Categorical: {len(CATEGORICAL_COLUMNS)}\")\n",
    "print(f\"\\nSamples: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Default target\n",
    "default_target = create_default_target(df)\n",
    "plot_class_distribution(\n",
    "    default_target,\n",
    "    class_names=['Good Credit', 'Bad Credit'],\n",
    "    ax=axes[0],\n",
    "    title='Credit Default (Binary)'\n",
    ")\n",
    "\n",
    "# Tier target\n",
    "tier_target = create_tier_target(df)\n",
    "plot_class_distribution(\n",
    "    tier_target,\n",
    "    class_names=['Low Risk', 'Medium Risk', 'High Risk'],\n",
    "    ax=axes[1],\n",
    "    title='Risk Tier (Multi-class)'\n",
    ")\n",
    "\n",
    "# Approval target\n",
    "approval_target = create_approval_target(df)\n",
    "plot_class_distribution(\n",
    "    approval_target,\n",
    "    class_names=['Denied', 'Approved'],\n",
    "    ax=axes[2],\n",
    "    title='Loan Approval (Binary)'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass imbalance ratios:\")\n",
    "print(f\"  Default: {default_target.value_counts()[1]/len(default_target):.1%} positive (bad credit)\")\n",
    "print(f\"  Approval: {approval_target.value_counts()[1]/len(approval_target):.1%} positive (approved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numerical Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(NUMERICAL_COLUMNS):\n",
    "    plot_distribution(df[col], ax=axes[i], kind='both', title=col.replace('_', ' ').title())\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df[NUMERICAL_COLUMNS].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_correlation_heatmap(df, columns=NUMERICAL_COLUMNS, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show categorical value counts for key features\n",
    "key_categorical = ['checking_account_status', 'credit_history', 'purpose', 'employment_since']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_categorical):\n",
    "    counts = df[col].value_counts()\n",
    "    axes[i].barh(counts.index, counts.values, color=COLOURS[i])\n",
    "    axes[i].set_xlabel('Count')\n",
    "    axes[i].set_title(col.replace('_', ' ').title())\n",
    "    axes[i].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "print(f\"Total missing values: {missing.sum()}\")\n",
    "\n",
    "if missing.sum() > 0:\n",
    "    print(\"\\nMissing values by column:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate encoding\n",
    "df_encoded, encoders = encode_categoricals(df, method='onehot')\n",
    "\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After one-hot encoding: {df_encoded.shape[1]}\")\n",
    "print(f\"\\nNew feature names (sample):\")\n",
    "print(df_encoded.columns[:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature engineering\n",
    "df_engineered = engineer_all_features(df)\n",
    "\n",
    "print(f\"Features after engineering: {df_engineered.shape[1]}\")\n",
    "print(f\"\\nNew features:\")\n",
    "new_cols = set(df_engineered.columns) - set(df.columns)\n",
    "print(list(new_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modelling\n",
    "data = prepare_data(\n",
    "    target_type='default',\n",
    "    encoding_method='onehot',\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale=True,\n",
    ")\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "X_train_scaled = data['X_train_scaled']\n",
    "X_test_scaled = data['X_test_scaled']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "trained_models = {}\n",
    "\n",
    "model_configs = [\n",
    "    ('logistic_regression', X_train_scaled, X_test_scaled),\n",
    "    ('random_forest', X_train.values, X_test.values),\n",
    "    ('xgboost', X_train.values, X_test.values),\n",
    "    ('svm', X_train_scaled, X_test_scaled),\n",
    "    ('knn', X_train_scaled, X_test_scaled),\n",
    "    ('neural_network', X_train_scaled, X_test_scaled),\n",
    "]\n",
    "\n",
    "for model_name, X_tr, X_te in model_configs:\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    if model_name == 'neural_network':\n",
    "        model = get_model(model_name, params={'epochs': 100})\n",
    "        model = train_model(model, X_tr, y_train.values)\n",
    "    else:\n",
    "        model = get_model(model_name)\n",
    "        model = train_model(model, X_tr, y_train)\n",
    "    \n",
    "    trained_models[model_name] = {\n",
    "        'model': model,\n",
    "        'X_test': X_te,\n",
    "    }\n",
    "\n",
    "print(\"\\nAll models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Metrics Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all models\n",
    "all_metrics = []\n",
    "\n",
    "for model_name, model_data in trained_models.items():\n",
    "    model = model_data['model']\n",
    "    X_te = model_data['X_test']\n",
    "    \n",
    "    if model_name == 'neural_network':\n",
    "        metrics = evaluate_model(model, X_te, y_test.values)\n",
    "    else:\n",
    "        metrics = evaluate_model(model, X_te, y_test)\n",
    "    \n",
    "    metrics['model'] = model_name\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(all_metrics).set_index('model')\n",
    "comparison_df = comparison_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"Model Comparison (sorted by F1 score):\")\n",
    "print(\"=\" * 60)\n",
    "comparison_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Metrics Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 comparison\n",
    "plot_model_comparison(comparison_df, metric='f1', ax=axes[0], title='Model Comparison: F1 Score')\n",
    "\n",
    "# AUC comparison\n",
    "plot_model_comparison(comparison_df, metric='auc', ax=axes[1], title='Model Comparison: AUC')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "plot_metrics_grouped_bar(\n",
    "    comparison_df,\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1', 'auc'],\n",
    "    ax=ax,\n",
    "    title='All Metrics Comparison'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ROC Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for i, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    model = model_data['model']\n",
    "    X_te = model_data['X_test']\n",
    "    \n",
    "    if model_name == 'neural_network':\n",
    "        plot_roc_curve(model, X_te, y_test.values, ax=ax, label=model_name, colour=COLOURS[i])\n",
    "    else:\n",
    "        plot_roc_curve(model, X_te, y_test, ax=ax, label=model_name, colour=COLOURS[i])\n",
    "\n",
    "ax.set_title('ROC Curves - All Models')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, model_data) in enumerate(trained_models.items()):\n",
    "    model = model_data['model']\n",
    "    X_te = model_data['X_test']\n",
    "    \n",
    "    if model_name == 'neural_network':\n",
    "        plot_confusion_matrix(\n",
    "            model, X_te, y_test.values,\n",
    "            class_names=['Good', 'Bad'],\n",
    "            ax=axes[i],\n",
    "            title=model_name.replace('_', ' ').title()\n",
    "        )\n",
    "    else:\n",
    "        plot_confusion_matrix(\n",
    "            model, X_te, y_test,\n",
    "            class_names=['Good', 'Bad'],\n",
    "            ax=axes[i],\n",
    "            title=model_name.replace('_', ' ').title()\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretability - SHAP Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "plot_feature_importance(\n",
    "    trained_models['random_forest']['model'],\n",
    "    feature_names=feature_names,\n",
    "    top_n=10,\n",
    "    ax=axes[0],\n",
    "    title='Random Forest - Feature Importance'\n",
    ")\n",
    "\n",
    "plot_feature_importance(\n",
    "    trained_models['xgboost']['model'],\n",
    "    feature_names=feature_names,\n",
    "    top_n=10,\n",
    "    ax=axes[1],\n",
    "    title='XGBoost - Feature Importance'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP for XGBoost (fast with TreeExplainer)\n",
    "print(\"Computing SHAP values for XGBoost...\")\n",
    "xgb_shap = compute_shap_values(\n",
    "    trained_models['xgboost']['model'],\n",
    "    trained_models['xgboost']['X_test'],\n",
    "    feature_names=feature_names,\n",
    "    max_samples=100\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_shap_summary(xgb_shap, plot_type='bar', max_display=15)\n",
    "plt.title('XGBoost - SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Task Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Risk Tier Classification (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tier classification data\n",
    "tier_data = prepare_data(\n",
    "    target_type='tier',\n",
    "    encoding_method='onehot',\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale=True,\n",
    ")\n",
    "\n",
    "# Train XGBoost on tier task\n",
    "tier_model = get_model('xgboost')\n",
    "tier_model = train_model(tier_model, tier_data['X_train'].values, tier_data['y_train'])\n",
    "\n",
    "# Evaluate\n",
    "tier_metrics = evaluate_model(\n",
    "    tier_model,\n",
    "    tier_data['X_test'].values,\n",
    "    tier_data['y_test'],\n",
    "    average='macro'\n",
    ")\n",
    "\n",
    "print(\"Risk Tier Classification (XGBoost):\")\n",
    "print(\"-\" * 40)\n",
    "for name, value in tier_metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"{name.capitalize():12} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for tier classification\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plot_confusion_matrix(\n",
    "    tier_model,\n",
    "    tier_data['X_test'].values,\n",
    "    tier_data['y_test'],\n",
    "    class_names=['Low Risk', 'Medium Risk', 'High Risk'],\n",
    "    ax=ax,\n",
    "    title='Risk Tier Classification - Confusion Matrix'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Loan Approval Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare approval data\n",
    "approval_data = prepare_data(\n",
    "    target_type='approval',\n",
    "    encoding_method='onehot',\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale=True,\n",
    ")\n",
    "\n",
    "# Train XGBoost on approval task\n",
    "approval_model = get_model('xgboost')\n",
    "approval_model = train_model(approval_model, approval_data['X_train'].values, approval_data['y_train'])\n",
    "\n",
    "# Evaluate\n",
    "approval_metrics = evaluate_model(\n",
    "    approval_model,\n",
    "    approval_data['X_test'].values,\n",
    "    approval_data['y_test']\n",
    ")\n",
    "\n",
    "print(\"Loan Approval Prediction (XGBoost):\")\n",
    "print(\"-\" * 40)\n",
    "for name, value in approval_metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"{name.capitalize():12} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    approval_model,\n",
    "    approval_data['X_test'].values,\n",
    "    approval_data['y_test'],\n",
    "    class_names=['Denied', 'Approved'],\n",
    "    ax=axes[0],\n",
    "    title='Loan Approval - Confusion Matrix'\n",
    ")\n",
    "\n",
    "plot_roc_curve(\n",
    "    approval_model,\n",
    "    approval_data['X_test'].values,\n",
    "    approval_data['y_test'],\n",
    "    ax=axes[1],\n",
    "    label='XGBoost'\n",
    ")\n",
    "axes[1].set_title('Loan Approval - ROC Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: Credit Default Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: UCI German Credit (1000 samples, {X_train.shape[1]} features)\")\n",
    "print(f\"\\nBest Model: {comparison_df.index[0]} (F1 = {comparison_df.iloc[0]['f1']:.4f})\")\n",
    "print(\"\\nModel Rankings (by F1 score):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (model_name, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "    print(f\"  {i}. {model_name:25} F1={row['f1']:.4f}  AUC={row['auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Model Performance**: Tree-based models (XGBoost, Random Forest) typically perform best on this tabular dataset\n",
    "\n",
    "2. **Interpretability vs Performance Trade-off**:\n",
    "   - Logistic Regression: Most interpretable, reasonable baseline\n",
    "   - XGBoost/Random Forest: Best performance, SHAP provides interpretability\n",
    "   - Neural Network: May underperform on small tabular datasets\n",
    "\n",
    "3. **Important Features**: Checking account status, credit history, and credit amount consistently appear as top predictors\n",
    "\n",
    "4. **Class Imbalance**: The dataset has ~30% bad credit risk, which affects model calibration\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For Production**: Use XGBoost with SHAP explanations for audit trail\n",
    "2. **For Regulatory Compliance**: Consider Logistic Regression for full interpretability\n",
    "3. **For Exploration**: Use the individual model notebooks for deep dives\n",
    "4. **For Better Performance**: Consider ensemble stacking or more feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "print(\"Saving models...\")\n",
    "for model_name, model_data in trained_models.items():\n",
    "    save_path = save_model(model_data['model'], model_name)\n",
    "    print(f\"  Saved: {model_name}\")\n",
    "\n",
    "print(\"\\nAll models saved to outputs/models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
