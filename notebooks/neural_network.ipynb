{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Credit Classification\n",
    "\n",
    "This notebook demonstrates a simple feedforward neural network for credit default prediction using the UCI German Credit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "**Neural Networks** are composed of layers of interconnected nodes (neurons) that learn hierarchical representations of the data through backpropagation.\n",
    "\n",
    "### Pros\n",
    "- Can learn complex, non-linear patterns\n",
    "- Flexible architecture (depth, width, activations)\n",
    "- Universal function approximators\n",
    "- Automatically learns feature representations\n",
    "- Scales well with data (benefits from more data)\n",
    "\n",
    "### Cons\n",
    "- Overkill for small/tabular datasets (tree-based models often perform better)\n",
    "- Requires more hyperparameter tuning\n",
    "- Less interpretable than simpler models\n",
    "- Prone to overfitting without regularisation\n",
    "- Computationally expensive to train\n",
    "\n",
    "### When to Use\n",
    "- When you have large amounts of data\n",
    "- For complex pattern recognition tasks\n",
    "- When other methods have been exhausted\n",
    "- To demonstrate deep learning capability in a portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from creditclass.preprocessing import prepare_data\n",
    "from creditclass.training import get_model, train_model, save_model, NeuralNetworkClassifier\n",
    "from creditclass.evaluation import (\n",
    "    evaluate_model,\n",
    "    compute_shap_values,\n",
    ")\n",
    "from creditclass.plots import (\n",
    "    set_plot_style,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall,\n",
    "    plot_calibration,\n",
    "    plot_shap_summary,\n",
    ")\n",
    "\n",
    "set_plot_style()\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data(\n",
    "    target_type='default',\n",
    "    encoding_method='onehot',\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scale=True,  # Neural networks benefit from scaling\n",
    ")\n",
    "\n",
    "X_train = data['X_train_scaled']\n",
    "X_test = data['X_test_scaled']\n",
    "y_train = data['y_train'].values\n",
    "y_test = data['y_test'].values\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('neural_network', params={\n",
    "    'hidden_sizes': [64, 32],\n",
    "    'dropout': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'random_state': RANDOM_STATE,\n",
    "})\n",
    "\n",
    "model = train_model(model, X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Architecture: Input({X_train.shape[1]}) -> 64 -> 32 -> Output(2)\")\n",
    "print(f\"Dropout: {model.dropout}\")\n",
    "print(f\"Epochs: {model.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "print(\"Performance Metrics:\")\n",
    "print(\"-\" * 30)\n",
    "for name, value in metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"{name.capitalize():12} {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    model, X_test, y_test,\n",
    "    class_names=['Good Credit', 'Bad Credit'],\n",
    "    ax=axes[0],\n",
    "    title='Neural Network - Confusion Matrix'\n",
    ")\n",
    "\n",
    "plot_roc_curve(model, X_test, y_test, ax=axes[1], label='Neural Network')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plot_precision_recall(model, X_test, y_test, ax=ax, label='Neural Network')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Comparison\n",
    "\n",
    "Let's compare different network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    {'name': 'Shallow (32)', 'hidden_sizes': [32]},\n",
    "    {'name': 'Medium (64-32)', 'hidden_sizes': [64, 32]},\n",
    "    {'name': 'Deep (128-64-32)', 'hidden_sizes': [128, 64, 32]},\n",
    "    {'name': 'Wide (128)', 'hidden_sizes': [128]},\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    nn = NeuralNetworkClassifier(\n",
    "        hidden_sizes=arch['hidden_sizes'],\n",
    "        epochs=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    nn.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(nn, X_test, y_test)\n",
    "    arch_results.append({'Architecture': arch['name'], **metrics})\n",
    "\n",
    "arch_df = pd.DataFrame(arch_results)\n",
    "print(arch_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability\n",
    "\n",
    "Neural networks are often called \"black boxes\", but SHAP can help explain their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values (using KernelExplainer - may be slow)\n",
    "print(\"Computing SHAP values (this may take a moment)...\")\n",
    "shap_data = compute_shap_values(model, X_test, feature_names=feature_names, max_samples=50)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_shap_summary(shap_data, plot_type='bar', max_display=15)\n",
    "plt.title('Neural Network - SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_values = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "dropout_results = []\n",
    "\n",
    "for dropout in dropout_values:\n",
    "    nn = NeuralNetworkClassifier(\n",
    "        hidden_sizes=[64, 32],\n",
    "        dropout=dropout,\n",
    "        epochs=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    nn.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(nn, X_test, y_test)\n",
    "    dropout_results.append({'Dropout': dropout, **metrics})\n",
    "\n",
    "dropout_df = pd.DataFrame(dropout_results)\n",
    "print(dropout_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(dropout_df['Dropout'], dropout_df['accuracy'], 'o-', label='Accuracy')\n",
    "ax.plot(dropout_df['Dropout'], dropout_df['f1'], 's-', label='F1')\n",
    "ax.plot(dropout_df['Dropout'], dropout_df['auc'], '^-', label='AUC')\n",
    "\n",
    "ax.set_xlabel('Dropout Rate')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Neural Network Performance vs. Dropout')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plot_calibration(model, X_test, y_test, ax=ax, label='Neural Network')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = save_model(model, 'neural_network')\n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Performance**: On this small tabular dataset, neural networks may not outperform tree-based methods\n",
    "2. **Architecture**: Deeper isn't always better - match complexity to data size\n",
    "3. **Regularisation**: Dropout helps prevent overfitting, especially important for small datasets\n",
    "4. **Interpretability**: SHAP provides valuable insights into feature importance\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- For tabular data with < 10,000 samples, consider tree-based models first\n",
    "- Use dropout and batch normalisation to prevent overfitting\n",
    "- Start with a simple architecture and increase complexity gradually\n",
    "- Monitor training/validation loss to detect overfitting early\n",
    "- Neural networks shine with larger datasets and complex patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
