# CreditClass Design Document

## Overview

**CreditClass** is a lightweight Python repository demonstrating classification techniques on the UCI German Credit dataset. It showcases six models across three credit-related problems (default prediction, risk tier classification, loan approval), with a focus on clean visualisations and interpretable results.

## Project Structure

```
CreditClass/
├── src/
│   └── creditclass/
│       ├── __init__.py
│       ├── preprocessing.py
│       ├── feature_engineering.py
│       ├── training.py
│       ├── evaluation.py
│       └── plots.py
├── notebooks/
│   ├── master.ipynb
│   ├── logistic_regression.ipynb
│   ├── random_forest.ipynb
│   ├── xgboost.ipynb
│   ├── svm.ipynb
│   ├── knn.ipynb
│   └── neural_network.ipynb
├── scripts/
│   └── generate_readme_dashboard.py
├── tests/
│   ├── __init__.py
│   ├── test_preprocessing.py
│   ├── test_feature_engineering.py
│   ├── test_training.py
│   └── test_evaluation.py
├── data/
│   └── raw/           # Downloaded dataset goes here
├── outputs/
│   ├── figures/       # Saved plots
│   └── models/        # Serialised trained models
├── docs/
│   └── plans/
├── pyproject.toml
├── requirements.txt   # Auto-generated
├── README.md
└── .gitignore
```

## Data & Classification Problems

### Dataset: UCI German Credit

The dataset contains 1,000 samples with 20 features (7 numerical, 13 categorical) describing credit applicants. Features include account status, credit history, loan purpose, employment duration, and personal details. The original target is binary (good/bad credit risk).

### Three Classification Problems

1. **Credit Default Prediction** (binary)
   - Target: Whether the applicant is a credit risk (1 = bad, 0 = good)
   - Uses the original dataset target directly
   - Primary problem; all models trained and compared here

2. **Credit Score Tier Classification** (multi-class)
   - Target: Risk category (low / medium / high)
   - Derived by combining the original target with features like credit amount, duration, and account status to create synthetic tiers
   - Demonstrates multi-class metrics (macro/micro F1, per-class precision/recall)

3. **Loan Approval Prediction** (binary)
   - Target: Approve / deny decision
   - Derived by applying business rules: e.g., approve if good risk AND credit amount below threshold AND stable employment
   - Demonstrates how the same features can serve different business objectives

## Models & Interpretability

### Six Models

| Model | Pros | Cons |
|-------|------|------|
| **Logistic Regression** | Interpretable coefficients, fast, good baseline, works well with regularisation | Assumes linear decision boundary, struggles with complex interactions |
| **Random Forest** | Handles non-linearity, robust to outliers, built-in feature importance | Can overfit on small datasets, less interpretable than linear models |
| **XGBoost** | State-of-the-art performance, handles missing values, regularisation built-in | Requires tuning, can be a black box, slower to train than simpler models |
| **SVM** | Effective in high dimensions, flexible kernels, strong theoretical foundation | Slow on large datasets, requires feature scaling, kernel choice matters |
| **k-NN** | Simple, no training phase, naturally handles multi-class | Slow at inference, sensitive to irrelevant features, needs distance metric tuning |
| **Neural Network** | Learns complex patterns, flexible architecture | Overkill for small/tabular data, requires more tuning, least interpretable |

### Interpretability

SHAP (SHapley Additive exPlanations) is applied to all models to provide:
- Global feature importance (which features matter most overall)
- Local explanations (why a specific prediction was made)
- Summary plots comparing feature impact across models

## Visualisations

### EDA Plots (master notebook)
- Feature distributions (histograms for numerical, bar charts for categorical)
- Correlation heatmap for numerical features
- Class imbalance bar chart for each target
- Missing value matrix (if any)

### Model Evaluation Plots (per-model notebooks + master)
- Confusion matrix with annotations
- ROC curve with AUC score
- Precision-recall curve with average precision
- Learning curves (train/validation score vs training size)
- Calibration curves (reliability diagrams)

### Comparison Plots (master notebook)
- Model comparison table (accuracy, precision, recall, F1, AUC)
- Grouped bar chart comparing metrics across models
- Feature importance comparison (top 10 features per model)
- SHAP summary plots (beeswarm and bar)

### README Dashboard (generated by script)
- Single polished figure combining:
  - ROC curves for all models overlaid
  - Bar chart of F1 scores
  - Top 5 features by importance
  - Small confusion matrix grid
- Saved as `outputs/figures/dashboard.png`

### Style Consistency
- Seaborn's `whitegrid` style
- Consistent colour palette (e.g., `tab10` or a custom muted palette)
- All figures saved at 300 DPI for crisp README rendering

## Core Scripts

### `preprocessing.py`
- `download_data()` - Fetches UCI German Credit dataset, caches locally in `data/raw/`
- `load_data()` - Reads raw data, returns pandas DataFrame
- `encode_categoricals()` - Label/one-hot encoding with consistent column naming
- `create_default_target()` - Returns original binary target
- `create_tier_target()` - Derives low/medium/high risk categories
- `create_approval_target()` - Derives approve/deny based on business rules
- `split_data()` - Train/test split with stratification

### `feature_engineering.py`
- `add_interaction_terms()` - Creates key feature interactions (e.g., amount × duration)
- `bin_numerical_features()` - Discretises continuous features into categories
- `get_feature_pipeline()` - Returns sklearn Pipeline for consistent transforms

### `training.py`
- `get_model()` - Factory function returning configured model by name
- `train_model()` - Fits model, returns trained estimator
- `tune_hyperparameters()` - Optional grid/random search wrapper
- `save_model()` / `load_model()` - Serialisation to `outputs/models/`

### `evaluation.py`
- `evaluate_model()` - Returns dict of metrics (accuracy, precision, recall, F1, AUC)
- `get_confusion_matrix()` - Wrapper for sklearn confusion matrix
- `compute_shap_values()` - SHAP explainer for any model type
- `compare_models()` - Aggregates metrics across models into DataFrame

### `plots.py`
- Functions for each plot type: `plot_confusion_matrix()`, `plot_roc_curve()`, `plot_precision_recall()`, `plot_learning_curve()`, `plot_calibration()`, `plot_shap_summary()`, `plot_feature_importance()`, `plot_correlation_heatmap()`, `plot_class_distribution()`, etc.
- All accept `ax` parameter for subplot integration
- Consistent `save_figure()` utility

## Notebooks

### Individual Model Notebooks

Each follows the same structure:
1. **Setup** - Imports from `creditclass` package
2. **Load data** - Preprocessed data via `preprocessing.py`
3. **Model overview** - Brief explanation, pros/cons, when to use
4. **Training** - Fit model on default prediction task
5. **Evaluation** - Metrics, confusion matrix, ROC, precision-recall
6. **Interpretability** - SHAP values and feature importance
7. **Hyperparameter tuning** - Optional section showing tuning impact
8. **Summary** - Key takeaways for this model

### Master Notebook (`master.ipynb`)

1. **Introduction** - Project overview, dataset description
2. **Data loading** - Download and inspect raw data
3. **EDA** - Distributions, correlations, missing values, class balance
4. **Preprocessing demo** - Show encoding and target derivation
5. **Model comparison** - Train all six models, collect metrics
6. **Results** - Comparison tables, grouped charts, ROC overlay
7. **Interpretability** - SHAP comparison across models
8. **Per-task analysis** - Brief results for tier classification and loan approval
9. **Conclusions** - Which model works best and why, trade-offs discussed

### Dashboard Script (`generate_readme_dashboard.py`)

- Imports trained models from `outputs/models/`
- Generates the composite dashboard figure
- Saves to `outputs/figures/dashboard.png`
- Can be run standalone: `python scripts/generate_readme_dashboard.py`

## Dependencies & Testing

### Core Dependencies

```
numpy
pandas
scikit-learn
xgboost
torch
shap
matplotlib
seaborn
requests
```

### Development Dependencies

```
pytest
pytest-cov
black
```

### Package Configuration

`pyproject.toml` defines the package with:
- Project metadata (name, version, description, author)
- Dependencies split into `[project.dependencies]` and `[project.optional-dependencies.dev]`
- Entry point for dashboard script (optional)

`requirements.txt` auto-generated via:
```bash
pip-compile pyproject.toml -o requirements.txt
```

### Tests

- `test_preprocessing.py` - Validates data loading, encoding, target creation, split ratios
- `test_feature_engineering.py` - Checks interaction terms, binning outputs correct shapes
- `test_training.py` - Verifies models train without error, save/load round-trips
- `test_evaluation.py` - Confirms metrics return expected types and ranges

Run with:
```bash
pytest tests/
```

## README Structure

1. **Header** - Project title, one-line description, dashboard image
2. **Overview** - What this repo demonstrates, who it's for
3. **Quick start** - Clone, install, run master notebook
4. **Dataset** - UCI German Credit description, link to source
5. **Models** - Table of all six models with brief pros/cons
6. **Project structure** - Directory tree with descriptions
7. **Usage**
   - Running individual model notebooks
   - Running the master notebook
   - Generating the dashboard figure
   - Running tests
8. **Results summary** - Key findings, best performing model
9. **Licence** - MIT

## Documentation Standards

- All functions have docstrings (Google style)
- Type hints on function signatures
- British English throughout (colour, analyse, summarise, standardise, etc.)
- Code comments only where logic is non-obvious
